{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "# Read from Delta table\n",
    "weather_stats = spark.read.table(\"weather_stats\")\n",
    "\n",
    "# Create temporary view for SQL queries\n",
    "weather_stats.createOrReplaceTempView(\"weather_stats\")\n",
    "\n",
    "# Example analysis queries\n",
    "# 1. Find temperature anomalies\n",
    "anomalies = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        sensor_id,\n",
    "        window,\n",
    "        temperature_avg,\n",
    "        humidity_avg,\n",
    "        wind_speed_avg\n",
    "    FROM weather_stats\n",
    "    WHERE temperature_avg > (\n",
    "        SELECT AVG(temperature_avg) + 2 * STDDEV(temperature_avg)\n",
    "        FROM weather_stats\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# 2. Calculate sensor reliability\n",
    "sensor_reliability = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        sensor_id,\n",
    "        COUNT(*) as reading_count,\n",
    "        AVG(temperature_avg) as avg_temperature,\n",
    "        STDDEV(temperature_avg) as temp_stability\n",
    "    FROM weather_stats\n",
    "    GROUP BY sensor_id\n",
    "    ORDER BY reading_count DESC\n",
    "\"\"\")\n",
    "\n",
    "# Display results using Databricks display() function\n",
    "display(anomalies)\n",
    "display(sensor_reliability)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
